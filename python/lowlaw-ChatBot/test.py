# ê·¸ëƒ¥ test
import streamlit as st
from sentence_transformers import SentenceTransformer, util
from elasticsearch import Elasticsearch
from PIL import Image

# Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
es_cloud_id = "lowlaw:YXAtbm9ydGhlYXN0LTIuYXdzLmVsYXN0aWMtY2xvdWQuY29tOjQ0MyQ2YzNmMjA4MmNiMzk0M2YxYTBiZWI0ZDY2M2JmM2VlZCRjZTA2NGZhNjFiMmI0N2Y0ODgzMjY0Y2FlMzVlZDgxZQ=="
es_username = "elastic"
es_pw = "LWkW2eILoZYZylsDDThLaCKY"

es = Elasticsearch(cloud_id=es_cloud_id, basic_auth=(es_username, es_pw))

# SentenceTransformer ëª¨ë¸ ë¡œë“œ
@st.cache_data() # st ìºì‹± (ë¡œì»¬ìºì‹œì— ì €ì¥)
def cached_model():
    model = SentenceTransformer('jhgan/ko-sroberta-multitask')
    return model

def load_image(img_file): # st ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
    img = Image.open(img_file)
    return img

model = cached_model() # sentenceBERT ëª¨ë¸

# Streamlit ì•± ì‹œì‘
st.title("LOWLAW")

if 'generated' not in st.session_state: # st generated ì´ˆê¸°í™” ì„¤ì •
    st.session_state['generated'] = []

if 'past' not in st.session_state: # st past ì´ˆê¸°í™” ì„¤ì •
    st.session_state['past'] = []

# ChatBotì˜ í™˜ì˜ì¸ì‚¬
with st.chat_message("assistant"):
    st.write("ì•ˆë…•í•˜ì„¸ìš”! LOWLAWì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤ğŸ‘‹")
    st.write("ë‚´ê°€ ì–´ë–¤ ìƒí™©ì¸ì§€ íŒŒì•…í•˜ê³  ì–´ë–»ê²Œ í•´ê²°í• ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´ LOWLAWì™€ ëŒ€í™”í•˜ë©´ì„œ í•´ê²°í•´ë³´ì„¸ìš”")

# ì‚¬ìš©ìì—ê²Œ ë¬¸ì¥ ì…ë ¥ ë°›ê¸°
user_input = st.chat_input(placeholder = "ì–´ë–¤ ìƒí™©ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”!")

# ë¬¸ì¥ë²¡í„° ê³„ì‚°
embeddings = model.encode([user_input])[0] if user_input is not None else None

# Elasticsearchì—ì„œ embedding í•„ë“œ ê°’ ê²€ìƒ‰
query = {
    "query": {
        "match_all": {}
    },
    "_source": ["question","answer","law", "prec", "embedding"]
}

response = es.search(index="legal_qa", body=query, size=100)

# ê°€ì¥ ë†’ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê°’ ì´ˆê¸°í™”
max_cosine_similarity = -1
best_answer = ""

if user_input : # ì‚¬ìš©ìê°€ user_inputë¥¼ ì…ë ¥í•˜ì˜€ë‹¤ë©´
    # ê° ë¬¸ì„œì™€ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ
    for hit in response["hits"]["hits"]:
        doc_embedding = hit["_source"]["embedding"]
    
        # Elasticsearchì—ì„œ ê°€ì ¸ì˜¨ 'embedding' ê°’ì„ ë¬¸ìì—´ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        doc_embedding = [float(value) for value in doc_embedding.strip("[]").split(", ")]
    
        cosine_similarity = util.pytorch_cos_sim(embeddings, [doc_embedding]).item()
    
        if cosine_similarity > max_cosine_similarity:
            max_cosine_similarity = cosine_similarity
            best_answer = hit["_source"]["answer"]
            related_law = hit["_source"].get("law", None) # í•„ë“œì— ë°ì´í„°ê°€ ì¡´ì¬í•˜ë©´ law ê°’ì„ ê°€ì ¸ì˜¤ê³  ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜
            related_prec = hit["_source"].get("prec", None)
        
    st.session_state.past.append(user_input) # ì‚¬ìš©ì user_input append
    st.session_state.generated.append(best_answer) # ê°€ì¥ ìœ ì‚¬í•œ ë‹µë³€ append


for i in range(len(st.session_state['past'])): # ë©”ì„¸ì§€ ë„ìš°ê¸°
    with st.chat_message("user"):
        st.write(st.session_state['past'][i])
    if len(st.session_state['generated']) > i:
        with st.chat_message("assistant"):
            st.write(st.session_state['generated'][i])
